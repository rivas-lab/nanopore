{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from logging.config import dictConfig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import collections as cl\n",
    "import bisect\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "logging_config = dict(\n",
    "    version = 1,\n",
    "    formatters = {\n",
    "        'f': {'format':\n",
    "              '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'}\n",
    "        },\n",
    "    handlers = {\n",
    "        'h': {'class': 'logging.StreamHandler',\n",
    "              'formatter': 'f',\n",
    "              'level': logging.DEBUG}\n",
    "        },\n",
    "    root = {\n",
    "        'handlers': ['h'],\n",
    "        'level': logging.DEBUG,\n",
    "        },\n",
    ")\n",
    "dictConfig(logging_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-01 11:26:06,556 notebook     INFO     hello\n"
     ]
    }
   ],
   "source": [
    "logger_nb = logging.getLogger('notebook')\n",
    "logger_nb.info('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_dir='/oak/stanford/groups/mrivas/public_data/nanopore-wgs-consortium/rel3/hg19/chr20'\n",
    "hap_basename='rel3.chr20.12500.10k-chr20impv1-keep-maf0005-snv-biallelic-geno01-hwe1e-10.head.hap'\n",
    "hap_f='{}/{}'.format(read_dir, hap_basename)\n",
    "\n",
    "data_dir='/oak/stanford/groups/mrivas/users/ytanigaw/nanopore-data'\n",
    "\n",
    "block_tsv_f='{}/{}'.format(\n",
    "    data_dir,\n",
    "    'chr20impv1-keep-maf0005-snv-biallelic-geno01-hwe1e-10-block-stronglow050-stronghigh083-infofrac10.tsv'\n",
    ")\n",
    "\n",
    "prior_count_dir='{}/{}'.format(data_dir, 'prior_count')\n",
    "log_likelihood_dir='{}/{}/{}'.format(data_dir, 'log_likelihood', hap_basename[:-4])\n",
    "posterior_dir='{}/{}/{}'.format(data_dir, 'posterior', hap_basename[:-4])\n",
    "if not os.path.exists(posterior_dir):\n",
    "    os.makedirs(posterior_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_prior_cnts(block_df, prior_count_dir):\n",
    "    \n",
    "    logger_cnt = logging.getLogger('read_prior_cnts')    \n",
    "    logger_cnt.info(\n",
    "        'reading prior counts from {}'.format(prior_count_dir)\n",
    "    )        \n",
    "    \n",
    "    prior_cnt_keys = [None] * len(block_df)\n",
    "    prior_cnt_vals = [None] * len(block_df)\n",
    "    for block_id in range(len(block_df)):\n",
    "        if(block_id % 100 == 0):\n",
    "            logger_cnt.info(\n",
    "                'reading block {} of {}'.format(block_id, len(block_df))\n",
    "            )    \n",
    "        cnt = np.load('{}/{}.npz'.format(prior_count_dir, block_id))\n",
    "        prior_cnt_keys[block_id] = cnt['keys']\n",
    "        prior_cnt_vals[block_id] = cnt['vals']\n",
    "        \n",
    "    logger_cnt.info(\n",
    "        'prior counts is loaded on memory'\n",
    "    )    \n",
    "        \n",
    "    return prior_cnt_keys, prior_cnt_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_log_likelihood(block_df, log_likelihood_dir):\n",
    "    \n",
    "    logger_ll = logging.getLogger('read_log_likelihood')\n",
    "    logger_ll.info(\n",
    "        'reading read_log_likelihood from {}'.format(log_likelihood_dir)\n",
    "    )        \n",
    "    \n",
    "    skipped_blocks = []\n",
    "    \n",
    "    log_likelihood = dict([])\n",
    "    for block_id in range(len(block_df)):\n",
    "        if(block_id % 100 == 0):\n",
    "            logger_ll.info(\n",
    "                'reading block {} of {}'.format(block_id, len(block_df))\n",
    "            )    \n",
    "        npz_file = '{}/ll{}.npz'.format(log_likelihood_dir, block_id)\n",
    "        if not os.path.isfile(npz_file):\n",
    "            skipped_blocks.append(block_id)\n",
    "        else:\n",
    "            ll = np.load(npz_file)\n",
    "            log_likelihood[block_id] = ll['ll']\n",
    "\n",
    "    if(len(skipped_blocks) > 0):\n",
    "        logger_ll.info(\n",
    "            'skipped blocks are {}'.format(skipped_blocks)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    logger_ll.info(\n",
    "        'log likelihood is loaded on memory'\n",
    "    )    \n",
    "        \n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_posterior_sub(prior_cnt_vals, log_likelihood, block_id):\n",
    "    prior_log_prob = np.log(\n",
    "        1.0 * prior_cnt_vals[block_id] / np.sum(prior_cnt_vals[block_id])\n",
    "    )\n",
    "\n",
    "    log_joint_prob = prior_log_prob + log_likelihood[block_id]\n",
    "\n",
    "    log_partition = np.log(np.sum(np.exp(log_joint_prob)))\n",
    "\n",
    "    log_posterior = log_joint_prob - log_partition    \n",
    "\n",
    "    return log_posterior\n",
    "\n",
    "def compute_log_posterior(prior_cnt_vals, log_likelihood):\n",
    "    logger_clp = logging.getLogger('compute_log_posterior')\n",
    "    logger_clp.info(\n",
    "        'computing log posterior probabilities'\n",
    "    )            \n",
    "    log_posterior = dict([])\n",
    "    for block_id in sorted(log_likelihood.keys()):\n",
    "        if(block_id % 100 == 0):\n",
    "            logger_clp.info(\n",
    "                'processing block {}'.format(block_id)\n",
    "            )            \n",
    "        log_posterior[block_id] = compute_log_posterior_sub(prior_cnt_vals, log_likelihood, block_id)\n",
    "        \n",
    "    return log_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "block_df = pd.read_csv(block_tsv_f, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-01 11:26:06,702 read_prior_cnts INFO     reading prior counts from /oak/stanford/groups/mrivas/users/ytanigaw/nanopore-data/prior_count\n",
      "2017-07-01 11:26:06,704 read_prior_cnts INFO     reading block 0 of 642\n",
      "2017-07-01 11:26:10,853 read_prior_cnts INFO     reading block 100 of 642\n",
      "2017-07-01 11:26:15,825 read_prior_cnts INFO     reading block 200 of 642\n",
      "2017-07-01 11:26:22,061 read_prior_cnts INFO     reading block 300 of 642\n",
      "2017-07-01 11:26:27,197 read_prior_cnts INFO     reading block 400 of 642\n",
      "2017-07-01 11:26:32,313 read_prior_cnts INFO     reading block 500 of 642\n",
      "2017-07-01 11:26:36,475 read_prior_cnts INFO     reading block 600 of 642\n",
      "2017-07-01 11:26:38,372 read_prior_cnts INFO     prior counts is loaded on memory\n"
     ]
    }
   ],
   "source": [
    "prior_cnt_keys, prior_cnt_vals = read_prior_cnts(block_df, prior_count_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-01 11:26:38,384 read_log_likelihood INFO     reading read_log_likelihood from /oak/stanford/groups/mrivas/users/ytanigaw/nanopore-data/prior_count\n",
      "2017-07-01 11:26:38,386 read_log_likelihood INFO     reading block 0 of 642\n",
      "2017-07-01 11:26:38,391 read_log_likelihood INFO     reading block 100 of 642\n",
      "2017-07-01 11:26:38,394 read_log_likelihood INFO     reading block 200 of 642\n",
      "2017-07-01 11:26:38,397 read_log_likelihood INFO     reading block 300 of 642\n",
      "2017-07-01 11:26:38,400 read_log_likelihood INFO     reading block 400 of 642\n",
      "2017-07-01 11:26:38,403 read_log_likelihood INFO     reading block 500 of 642\n",
      "2017-07-01 11:26:38,406 read_log_likelihood INFO     reading block 600 of 642\n",
      "2017-07-01 11:26:38,408 read_log_likelihood INFO     skipped blocks are [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641]\n",
      "2017-07-01 11:26:38,410 read_log_likelihood INFO     log likelihood is loaded on memory\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = read_log_likelihood(block_df, log_likelihood_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-01 11:26:38,418 compute_log_posterior INFO     computing log posterior probabilities\n",
      "2017-07-01 11:26:38,420 compute_log_posterior INFO     processing block 0\n"
     ]
    }
   ],
   "source": [
    "log_posterior = compute_log_posterior(prior_cnt_vals, log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for block_id, lp in log_posterior.items():\n",
    "    np.savez(\n",
    "        '{}/{}.npz'.format(posterior_dir, block_id), \n",
    "        lp = lp\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev27",
   "language": "python",
   "name": "dev27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
